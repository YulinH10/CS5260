{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qJB_qhlZk9C"
   },
   "source": [
    "# Assignment 2: Diffusion Model for Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRkFhWZitVXb"
   },
   "source": [
    "## Meta Instructions\n",
    "1. Environment: Install pytorch >= 1.7.0 and torchvision >= 0.8.0 to avoid any issues caused by package versions.\n",
    "\n",
    "2. Finish coding tasks according to instructions in this file. **Only** change the code the code within the regions marked by “Your code starts here” and “Your code ends here”. Do not modify anywhere else including comments. This task does not require GPUs and can meet the basic requirements.\n",
    "\n",
    "3. Please use the trained diffusion model to generate an image that contains the string of number in your StuID (e.g., if your StuID is A0123456J, please generate 0123456) and save it as a single jpg file, it is encouraged that you train the diffusion model by yourself. If you cannot find the computing resources, you can try to use this pretrained model for generating locally. (https://drive.google.com/file/d/1-9dozojlZxdpkei5lyxFeKofKYq4rQIf/view?usp=sharing)\n",
    "\n",
    "4. Submission: submit a zip file named \"StuID.zip\" (e.g., \"A0123456J.zip\") to Canvas **Assignments -> Assignment2**. Note that it is **NOT** NUSNET ID. The zip file should **only** include \"StuID_Assignment_2.ipynb\", \"StuID_Assignment_2.pdf\" and \"StuID_Assignment_2.jpg\". The submissison deadline is **23:59 on Feb 21**.\n",
    "\n",
    "\n",
    "### Optional Puzzles for Extra Credits\n",
    "\n",
    "You are able to get a full marks of this assignment by correctly completing and submitting the code and PDF report according to the above requirements. However, if you want to earn 1-2 bonus points in your total assignment grade (not exceeding the maximum score), you can choose to solve one of the following optional puzzles and include it in your final codes and reports:\n",
    "\n",
    "1. Puzzle 1: How to generate a two-digit image using an end-to-end diffusion model? (e.g., generate 21 directly)\n",
    "2. Puzzle 2: Besides simply sampling the noise from a Gaussian distribution, what are some alternative sampling strategies that could improve the generation process? (you can refer to https://arxiv.org/pdf/2501.09732)\n",
    "\n",
    "\n",
    "### For any questions, please do one of the following actions with priority:\n",
    "1. Search for similar questions on Slack (https://app.slack.com/client/T088V95D8LC/C088L557RK8).\n",
    "2. Propose a new question on Slack if not already answered.\n",
    "3. For private inquiries, e-mail to Pengfei Zhou (e1374451@u.nus.edu) and Xiangyan Liu (e0950125@u.nus.edu) with the subject starting with \"CS5260 2025 Spring\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhIgGq3za0yh"
   },
   "source": [
    "\n",
    "# Assignment 3: Diffusion Model for Image Generation\n",
    "\n",
    "Diffusion Models, including the Denoising Diffusion Probabilistic Model (SimpleDiffusion), represent a powerful class of generative models that simulate the gradual process of diffusing data into noise and then denoising it back into coherent samples.\n",
    "\n",
    "Inspired by the natural diffusion process observed in physics, SimpleDiffusion operates in two phases: a forward phase where data is incrementally noised until it becomes indistinguishable from random noise, and a reverse phase where this process is inverted, gradually denoising to generate new data samples that closely mimic the original data distribution.\n",
    "\n",
    "This innovative approach allows SimpleDiffusion to produce high-quality, diverse outputs across various domains such as images, audio, and text, without the adversarial training complexities associated with other generative models.\n",
    "\n",
    "**References:**\n",
    "- Denoising Diffusion Probabilistic Models (SimpleDiffusion) Paper:  [DDPM](https://arxiv.org/abs/2006.11239)\n",
    "- Blog about Diffusion models: [DDPM Blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n",
    "\n",
    "This assignment requires you to implement a simple DDPM-based diffusion model for generating handwritten digits images (32×32×3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLHSIArLcFK0"
   },
   "source": [
    "## Import Packages\n",
    "\n",
    "It is to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LQnlc27k7Aiw"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.makedirs('./data/diffusion_outputs10', exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt6JSKawk7_b"
   },
   "source": [
    "## Create dataset\n",
    "\n",
    "Load the MNIST dataset and apply the following preprocessing:\n",
    "\n",
    "Resize images to image_size×image_size (you can use the torchvision.transforms.Resize function).\n",
    "Normalize pixel values to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uuckjpW_k1LN"
   },
   "outputs": [],
   "source": [
    "def create_mnist_dataloaders(batch_size, image_size=32, num_workers=4):\n",
    "\n",
    "    ################################\n",
    "    # Your code starts here\n",
    "    ################################\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        #[-1, 1]\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # Your code ends here\n",
    "    ################################\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"./mnist_data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=preprocess\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"./mnist_data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocess\n",
    "    )\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers),\\\n",
    "           DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buW6BaNga-XH"
   },
   "source": [
    "\n",
    "# Model Architecture: U-Net\n",
    "\n",
    "Build a neural network to predict noise. You can use a U-Net or any convolutional architecture of your choice. The network should take as input the noisy image along with the time-step information and output the predicted noise. Make sure that:\n",
    "\n",
    "- The network handles MNIST images (e.g., 32×32).\n",
    "\n",
    "- The network has a bottleneck structure with multiple downsampling and upsampling layers.\n",
    "\n",
    "- The network includes residual connections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DuJCCZ5dInQq"
   },
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        ################################\n",
    "        # Your code starts here\n",
    "        ################################\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        if self.is_res:\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "        ################################\n",
    "        # Your code ends here\n",
    "        ################################\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "\n",
    "    ################################\n",
    "    # Your code starts here\n",
    "    ################################\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # Your code ends here\n",
    "    ################################\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "\n",
    "    ################################\n",
    "    # Your code starts here\n",
    "    ################################\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    ################################\n",
    "    # Your code ends here\n",
    "    ################################\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 8, 8), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep,\n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1,self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        #up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B9GlZrotBXy"
   },
   "source": [
    "## The Diffusion Model\n",
    "\n",
    "According to DDPM, define a noise schedule (e.g., a linear schedule) and implement the forward diffusion process. Given an original image, generate a noisy image at a randomly chosen time step t.\n",
    "\n",
    "Implement the noise addition process and reverse process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7kuMCXuYutgZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X7TKWoZpInQs"
   },
   "outputs": [],
   "source": [
    "class SimpleDiffusion(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(SimpleDiffusion, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "\n",
    "        x_t = (\n",
    "        ################################\n",
    "        # Your code starts here\n",
    "        ################################\n",
    "\n",
    "        self.sqrtab[_ts,None,None,None]*x + self.sqrtmab[_ts,None,None,None]*noise\n",
    "\n",
    "        ################################\n",
    "        # Your code ends here\n",
    "        ################################\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w = 0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something\n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "\n",
    "            ################################\n",
    "            # Your code starts here\n",
    "            ################################\n",
    "            x_i = (self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i]) + self.sqrt_beta_t[i] * z)\n",
    "\n",
    "\n",
    "            ################################\n",
    "            # Your code ends here\n",
    "            ################################\n",
    "\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "\n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWJUjFIHInQt"
   },
   "source": [
    "## Trainer\n",
    "\n",
    "Including the main function.\n",
    "\n",
    "You should complete the inference code that uses the trained model to generate desired images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ed12NNXPtDon"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_mnist():\n",
    "\n",
    "    imagesize = 32\n",
    "    # hardcoding these here\n",
    "    n_epoch = 20\n",
    "    batch_size = 256\n",
    "    n_T = 400 # 500\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n_classes = 10\n",
    "    n_feat = 128 # 128 ok, 256 better (but slower)\n",
    "    lrate = 1e-4\n",
    "    save_model = False\n",
    "    save_dir = './data/diffusion_outputs10/'\n",
    "    ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
    "\n",
    "    ddpm = SimpleDiffusion(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "    ddpm.to(device)\n",
    "\n",
    "    # optionally load a model\n",
    "    # ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
    "\n",
    "    dataloader, _ = create_mnist_dataloaders(batch_size=batch_size, image_size=imagesize, num_workers=4)\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "        print(f'epoch {ep}')\n",
    "        ddpm.train()\n",
    "\n",
    "        # linear lrate decay\n",
    "        optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "        pbar = tqdm(dataloader)\n",
    "        loss_ema = None\n",
    "        for x, c in pbar:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            loss = ddpm(x, c)\n",
    "            loss.backward()\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "            optim.step()\n",
    "\n",
    "        # for eval, save an image of currently generated samples (top rows)\n",
    "        # followed by real images (bottom rows)\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            n_sample = 4*n_classes\n",
    "            for w_i, w in enumerate(ws_test):\n",
    "                x_gen, x_gen_store = ddpm.sample(n_sample, (1, 32, 32), device, guide_w=w)\n",
    "\n",
    "                # append some real images at bottom, order by class also\n",
    "                x_real = torch.Tensor(x_gen.shape).to(device)\n",
    "                for k in range(n_classes):\n",
    "                    for j in range(int(n_sample/n_classes)):\n",
    "                        try:\n",
    "                            idx = torch.squeeze((c == k).nonzero())[j]\n",
    "                        except:\n",
    "                            idx = 0\n",
    "                        x_real[k+(j*n_classes)] = x[idx]\n",
    "\n",
    "                x_all = torch.cat([x_gen, x_real])\n",
    "                grid = make_grid(x_all*-1 + 1, nrow=10)\n",
    "                save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "                print('saved image at ' + save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "\n",
    "                if ep%5==0 or ep == int(n_epoch-1):\n",
    "                    # create gif of images evolving over time, based on x_gen_store\n",
    "                    fig, axs = plt.subplots(nrows=int(n_sample/n_classes), ncols=n_classes,sharex=True,sharey=True,figsize=(8,3))\n",
    "                    def animate_diff(i, x_gen_store):\n",
    "                        print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n",
    "                        plots = []\n",
    "                        for row in range(int(n_sample/n_classes)):\n",
    "                            for col in range(n_classes):\n",
    "                                axs[row, col].clear()\n",
    "                                axs[row, col].set_xticks([])\n",
    "                                axs[row, col].set_yticks([])\n",
    "                                # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
    "                                plots.append(axs[row, col].imshow(-x_gen_store[i,(row*n_classes)+col,0],cmap='gray',vmin=(-x_gen_store[i]).min(), vmax=(-x_gen_store[i]).max()))\n",
    "                        return plots\n",
    "                    ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])\n",
    "                    ani.save(save_dir + f\"gif_ep{ep}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "                    print('saved image at ' + save_dir + f\"gif_ep{ep}_w{w}.gif\")\n",
    "        # optionally save model\n",
    "        if save_model and ep == int(n_epoch-1):\n",
    "            torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "            print('saved model at ' + save_dir + f\"model_{ep}.pth\")\n",
    "\n",
    "def generate_samples(model_path, save_dir, n_samples=40, image_size=(1, 32, 32), device=\"cuda\"):\n",
    "    device = torch.device(device)\n",
    "    ddpm = SimpleDiffusion(nn_model=ContextUnet(in_channels=1, n_feat=128, n_classes=10),betas=(1e-4, 0.02),n_T=400,device=device,drop_prob=0.1)\n",
    "    ddpm.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    ddpm.to(device)\n",
    "    ddpm.eval()\n",
    "\n",
    "    student_id = \"0297375\"\n",
    "    digits = [int(d) for d in student_id]\n",
    "    n_digits = len(digits)\n",
    "\n",
    "    samples = []\n",
    "    with torch.no_grad(): # do not compute grad when not training\n",
    "        for digit in digits:\n",
    "            # belike rewrite sample()\n",
    "            c_i = torch.tensor([digit]).to(device)\n",
    "            context_mask = torch.zeros_like(c_i).to(device)\n",
    "            c_i = c_i.repeat(2)\n",
    "            context_mask = context_mask.repeat(2)\n",
    "            context_mask[1:] = 1.  # makes second half of batch context free\n",
    "            x_i = torch.randn(1, *image_size).to(device)\n",
    "\n",
    "            for i in range(ddpm.n_T, 0, -1):\n",
    "                print(f'sampling timestep {i} for digit {digit}', end='\\r')\n",
    "                t_is = torch.tensor([i / ddpm.n_T]).to(device)\n",
    "                t_is = t_is.repeat(1, 1, 1, 1)\n",
    "\n",
    "                # Double batch\n",
    "                x_i_double = x_i.repeat(2, 1, 1, 1)\n",
    "                t_is = t_is.repeat(2, 1, 1, 1)\n",
    "\n",
    "                # Noise prediction\n",
    "                z = torch.randn(1, *image_size).to(device) if i > 1 else 0\n",
    "                eps = ddpm.nn_model(x_i_double, c_i, t_is, context_mask)\n",
    "                eps1 = eps[:1]\n",
    "                eps2 = eps[1:]\n",
    "                guide_w = 0.5  # guidance using 0.5, 0 and 2 also works\n",
    "                eps = (1 + guide_w) * eps1 - guide_w * eps2\n",
    "\n",
    "                x_i = ( ddpm.oneover_sqrta[i] * (x_i - eps * ddpm.mab_over_sqrtmab[i]) + ddpm.sqrt_beta_t[i] * z )\n",
    "\n",
    "            samples.append(x_i)\n",
    "\n",
    "    x_gen = torch.cat(samples, dim=0)\n",
    "    grid = make_grid(x_gen * -1 + 1, nrow=n_digits)\n",
    "    save_image(grid, save_dir + \"generated_samples.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    set_seed()\n",
    "    #train_mnist()\n",
    "\n",
    "    # Please use the generate function to generate your desired digit images.\n",
    "    ################################\n",
    "    # Your code starts here\n",
    "    ################################\n",
    "\n",
    "    generate_samples(\"./model_19.pth\", \"./data/diffusion_outputs10/\", device=\"cpu\")\n",
    "\n",
    "    ################################\n",
    "    # Your code ends here\n",
    "    ################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 1\n",
    "How to generate a two-digit image using an end-to-end diffusion model？\n",
    "\n",
    "* The key challenge is adapting the model to handle a 2-digit label space (00-99) instead of single digits (0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2GGGM-16tVXi",
    "outputId": "21b0a575-6c9f-425f-ce08-6c80d404405d"
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=100):  # Changed n_classes to 100\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 8, 8),\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # Modified to handle two-digit numbers\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # Convert context to one hot embedding for 100 classes\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "\n",
    "        # Mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1, self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask))\n",
    "        c = c * context_mask\n",
    "\n",
    "        # Embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY2UHVOyXDz2"
   },
   "source": [
    "## Puzzle 2\n",
    "Besides simply sampling the noise from a Gaussian distribution, what are some alternative sampling strategies that could improve the generation process?？\n",
    "\n",
    "* Noise Selection: Select the most stable one from K random noises, sample the noise, computes stability score using cosine similarity, and retain the most stable noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class NoiseSelectionDiffusion(SimpleDiffusion):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super().__init__(nn_model, betas, n_T, device, drop_prob)\n",
    "\n",
    "    def noise_selection(self, K, c, size, device):\n",
    "        best_noise, best_stability = None, -1\n",
    "        for _ in range(K):\n",
    "            noise = torch.randn(1, *size).to(device)\n",
    "            stability = self.compute_inversion_stability(noise, c, size)\n",
    "            if stability > best_stability:\n",
    "                best_stability, best_noise = stability, noise\n",
    "        return best_noise\n",
    "\n",
    "    def compute_inversion_stability(self, noise, c, size):\n",
    "        x_0 = self.sample_from_noise(noise, c, size)\n",
    "        inverse_noise = self.inverse_noise(x_0, c)\n",
    "        return F.cosine_similarity(noise.view(noise.shape[0], -1),\n",
    "                                    inverse_noise.view(inverse_noise.shape[0], -1),\n",
    "                                    dim=1)\n",
    "\n",
    "    def sample_from_noise(self, noise, c, size):\n",
    "        x_i = noise\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            t_is = torch.tensor([i / self.n_T]).to(self.device).repeat(noise.shape[0], 1, 1, 1)\n",
    "            eps = self.nn_model(x_i, c, t_is, torch.zeros_like(c).to(self.device))\n",
    "            z = torch.randn_like(x_i) if i > 1 else 0\n",
    "            x_i = (self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i]) +\n",
    "                    self.sqrt_beta_t[i] * z)\n",
    "        return x_i\n",
    "\n",
    "    def inverse_noise(self, x_0, c):\n",
    "        x_t = x_0\n",
    "        for i in range(1, self.n_T + 1):\n",
    "            t_is = torch.tensor([i / self.n_T]).to(self.device).repeat(x_0.shape[0], 1, 1, 1)\n",
    "            x_t = (self.sqrtab[i] * x_t + self.sqrtmab[i] * torch.randn_like(x_t))\n",
    "        return x_t\n",
    "\n",
    "def generate_samples(model_path, save_dir, K=100, device=\"cuda\"):\n",
    "    device = torch.device(device)\n",
    "    ddpm = NoiseSelectionDiffusion(\n",
    "        nn_model=ContextUnet(in_channels=1, n_feat=128, n_classes=10),\n",
    "        betas=(1e-4, 0.02),\n",
    "        n_T=400,\n",
    "        device=device\n",
    "    )\n",
    "    #... all the same as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
