{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Training Language Models Using DeepSpeed\n",
    "## Overview\n",
    "In this assignment, you will explore DeepSpeed, a powerful deep learning optimization library developed by Microsoft. DeepSpeed enables training of extreme-scale models through various optimization techniques including ZeRO (Zero Redundancy Optimizer), mixed precision training, model parallelism, and memory offloading. You will configure DeepSpeed for different scenarios and analyze the performance impacts.\n",
    "\n",
    "## Objectives\n",
    "- Understand DeepSpeed's optimization techniques\n",
    "- Configure DeepSpeed for various performance optimization scenarios\n",
    "- Measure and analyze performance metrics across different configurations\n",
    "- Apply the knowledge to train larger models efficiently\n",
    "\n",
    "## Environment Setup\n",
    "1. Install the required package:\n",
    "   ```\n",
    "   pip install deepspeed>=0.6.5\n",
    "   ```\n",
    "2. Other typically required packages include PyTorch, transformers, accelerate, etc.\n",
    "3. Additional packages may be required depending on your implementation.\n",
    "\n",
    "## Grading Criteria\n",
    "- Correct understanding and explanation of DeepSpeed concepts (40%)\n",
    "- Proper implementation and configuration of DeepSpeed (30%)\n",
    "- Thorough analysis of experimental results with metrics (20%)\n",
    "- Report quality and clarity (10%)\n",
    "\n",
    "Successfully answering all questions with thorough explanations and corresponding experimental records, and submitting a well-structured report that demonstrates your reasoning will earn you 8/10 of the total score. There is no optional puzzle this time.\n",
    "\n",
    "### Reference Materials:  \n",
    "1. [official docs](https://deepspeed.readthedocs.io/en/latest/)  \n",
    "2. [hugging face](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)\n",
    "\n",
    "### For any questions, please do one of the following actions in order of priority:  \n",
    "1. Search for similar questions on Slack (https://app.slack.com/client/T088V95D8LC/C088L557RK8).  \n",
    "2. Post a new question on Slack if it has not already been answered.  \n",
    "3. For non-public questions, email Xiangyan Liu (e0950125@u.nus.edu) and Pengfei Zhou (e1374451@u.nus.edu) with the subject starting with \"CS5260 2025 Spring\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks and Questions\n",
    "\n",
    "For each of the following questions, you need to:\n",
    "1. Explain the concept in detail\n",
    "2. Demonstrate the configuration in a DeepSpeed config file\n",
    "3. Run experiments and record relevant metrics (training speed, GPU memory usage, CPU utilization, etc.)\n",
    "4. Analyze the results and provide insights\n",
    "\n",
    "> **Note:** You do not need to train the models on the full dataset, training for a short duration and documenting the necessary metrics is sufficient. Also, evaluation is not required.\n",
    "\n",
    "### Question 1: ZeRO Optimization Stages\n",
    "What are the different ZeRO optimization stages in DeepSpeed? What are the characteristics of each stage?\n",
    "\n",
    "**Note:** Experiment with different ZeRO stages and measure their impact on memory usage and training speed.\n",
    "\n",
    "### Question 2: Mixed Precision Training\n",
    "How to configure mixed precision training in DeepSpeed? \n",
    "\n",
    "**Note:** Compare the performance of FP32, FP16, and BF16 (if available) training in terms of memory usage, training speed, and convergence (optional).\n",
    "\n",
    "### Question 3: Model Parallelism\n",
    "How to configure DeepSpeed to implement model parallelism rather than just data parallelism?\n",
    "\n",
    "**Note:** Demonstrate the configuration and analyze how it affects training large models across multiple GPUs.\n",
    "\n",
    "### Question 4: Offload Techniques\n",
    "What are DeepSpeed's offload techniques? How to enable them in the configuration?\n",
    "\n",
    "**Note:** Experiment with offloading to CPU and/or NVMe and analyze the trade-offs between memory usage and training speed.\n",
    "\n",
    "### Question 5: Training Larger Models\n",
    "Using the knowledge gained from the previous questions, configure DeepSpeed to train a larger model (from the [pythia collection](https://huggingface.co/collections/EleutherAI/pythia-scaling-suite-64fb5dfa8c21ebb3db7ad2e1)) than would be possible with standard training approaches. Try to maximize model size while maintaining reasonable training speed.\n",
    "\n",
    "**Note:** Demonstrate your approach, configuration, and analyze the results in terms of model size, memory usage, and training speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:26:33.617452Z",
     "iopub.status.busy": "2025-04-25T14:26:33.617174Z",
     "iopub.status.idle": "2025-04-25T14:28:09.538341Z",
     "shell.execute_reply": "2025-04-25T14:28:09.537375Z",
     "shell.execute_reply.started": "2025-04-25T14:26:33.617429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.16.7.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.5.1+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->deepspeed) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->deepspeed) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->deepspeed) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->deepspeed) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->deepspeed) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->deepspeed) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->deepspeed)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->deepspeed)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->deepspeed)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->deepspeed)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->deepspeed)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->deepspeed)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->deepspeed)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->deepspeed) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->deepspeed) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->deepspeed) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->deepspeed) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->deepspeed) (2024.2.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.16.7-py3-none-any.whl size=1642797 sha256=95ca141a08cd77f9006f84bbe9701ce65d58236d47106ee07c2dcc4055dd7a95\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/e7/1a/2106f7197cc13e09c68f1b4f55f7e5117a985e726378968970\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: hjson, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepspeed\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deepspeed-0.16.7 hjson-3.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip3 install deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is a demonstration case showing how to enable DeepSpeed during LLM training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSpeed Experiment Scripts and Configurations\n",
    "# -----------------------------------------------\n",
    "# This file bundle includes:\n",
    "# 1) train_deepspeed.py - Main training script with metric logging and time-based stopping\n",
    "# 2) DeepSpeed JSON config files for five experiments:\n",
    "#    - Zero Optimization Stages (0,1,2,3)\n",
    "#    - Mixed Precision (FP32, FP16, BF16)\n",
    "#    - Model Parallelism\n",
    "#    - Offload Techniques (CPU, NVMe)\n",
    "#    - Large Model Training\n",
    "\n",
    "# ======== train_deepspeed.py ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:28:09.555214Z",
     "iopub.status.busy": "2025-04-25T14:28:09.554952Z",
     "iopub.status.idle": "2025-04-25T14:28:10.582391Z",
     "shell.execute_reply": "2025-04-25T14:28:10.581416Z",
     "shell.execute_reply.started": "2025-04-25T14:28:09.555188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_deepspeed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_deepspeed.py\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "import threading\n",
    "import psutil\n",
    "import argparse\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# Global flag for logger threads\n",
    "global logging_active\n",
    "logging_active = True\n",
    "\n",
    "def gpu_usage_logger(logfile=\"gpu_usage.csv\", interval=2, start_time=None):\n",
    "    global logging_active\n",
    "    if start_time is None:\n",
    "        start_time = time.time()\n",
    "    with open(logfile, \"w\") as f:\n",
    "        f.write(\"elapsed_time,gpu_index,gpu_utilization,memory_utilization,memory_used_MB,memory_total_MB\\n\")\n",
    "    while logging_active:\n",
    "        elapsed = time.time() - start_time\n",
    "        try:\n",
    "            cmd = [\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=index,utilization.gpu,utilization.memory,memory.used,memory.total\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            lines = result.stdout.strip().split(\"\\n\")\n",
    "            with open(logfile, \"a\") as f:\n",
    "                for line in lines:\n",
    "                    f.write(f\"{elapsed:.2f},\" + line + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(\"[GPU Logger]\", e)\n",
    "        time.sleep(interval)\n",
    "\n",
    "def cpu_usage_logger(logfile=\"cpu_usage.csv\", interval=2, start_time=None):\n",
    "    global logging_active\n",
    "    if start_time is None:\n",
    "        start_time = time.time()\n",
    "    with open(logfile, \"w\") as f:\n",
    "        f.write(\"elapsed_time,cpu_usage_percent\\n\")\n",
    "    psutil.cpu_percent(interval=None)\n",
    "    while logging_active:\n",
    "        elapsed = time.time() - start_time\n",
    "        try:\n",
    "            cpu = psutil.cpu_percent(interval=None)\n",
    "            with open(logfile, \"a\") as f:\n",
    "                f.write(f\"{elapsed:.2f},{cpu}\\n\")\n",
    "        except Exception as e:\n",
    "            print(\"[CPU Logger]\", e)\n",
    "        time.sleep(interval)\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "import time\n",
    "\n",
    "class StepTimeCallback(TrainerCallback):\n",
    "    def __init__(self, logfile=\"step_times.csv\"):\n",
    "        super().__init__()\n",
    "        self.logfile = logfile\n",
    "        # these will get their first real value at train-begin\n",
    "        self.start_time = None\n",
    "        self.step_start = None\n",
    "\n",
    "        # write header once\n",
    "        with open(self.logfile, \"w\") as f:\n",
    "            f.write(\"elapsed_time,step_time\\n\")\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        # set both timers at exactly the same baseline\n",
    "        now = time.time()\n",
    "        self.start_time = now\n",
    "        self.step_start = now\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        # mark the beginning of *this* training step\n",
    "        self.step_start = time.time()\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        now = time.time()\n",
    "\n",
    "        # guard against uninitialized timers\n",
    "        if self.start_time is None:\n",
    "            self.start_time = now\n",
    "        if self.step_start is None:\n",
    "            # nothing to log this round, but seed it for next time\n",
    "            self.step_start = now\n",
    "            return\n",
    "\n",
    "        step_time = now - self.step_start\n",
    "        elapsed   = now - self.start_time\n",
    "\n",
    "        with open(self.logfile, \"a\") as f:\n",
    "            f.write(f\"{elapsed:.2f},{step_time:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Callback to stop training after a max duration (seconds)\n",
    "class TimeBasedStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, max_duration):\n",
    "        super().__init__()\n",
    "        self.max_duration = max_duration\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        self.step_start = self.start_time\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.start_time and (time.time() - self.start_time) > self.max_duration:\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "# Compute summary metrics after training\n",
    "def compute_summary(prefix, gpu_csv, cpu_csv, speed_csv, summary_file):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        df_gpu = pd.read_csv(gpu_csv).sort_values(\"elapsed_time\")\n",
    "        df_cpu = pd.read_csv(cpu_csv).sort_values(\"elapsed_time\")\n",
    "        df_speed = pd.read_csv(speed_csv)\n",
    "\n",
    "        avg_gpu_mem = df_gpu[\"memory_used_MB\"].mean()\n",
    "        avg_gpu_util = df_gpu[\"gpu_utilization\"].mean()\n",
    "        avg_cpu = df_cpu[\"cpu_usage_percent\"].mean()\n",
    "        avg_step_time = df_speed[\"step_time\"].mean()\n",
    "        steps_per_sec = 1.0 / avg_step_time if avg_step_time > 0 else 0.0\n",
    "\n",
    "        with open(summary_file, \"w\") as f:\n",
    "            f.write(f\"Experiment {prefix} Summary:\\n\")\n",
    "            f.write(f\"Avg GPU Memory Used (MB): {avg_gpu_mem:.2f}\\n\")\n",
    "            f.write(f\"Avg GPU Utilization (%): {avg_gpu_util:.2f}\\n\")\n",
    "            f.write(f\"Avg CPU Usage (%): {avg_cpu:.2f}\\n\")\n",
    "            f.write(f\"Avg Step Time (s): {avg_step_time:.4f}\\n\")\n",
    "            f.write(f\"Est. Steps/sec: {steps_per_sec:.2f}\\n\")\n",
    "        print(f\"Written summary to {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Summary Error] Could not compute summary for {prefix}: {e}\")\n",
    "\n",
    "# Main training function\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--deepspeed_config\", type=str,\n",
    "        default=\"ds_config.json\",\n",
    "        help=\"Path to DeepSpeed JSON config\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_time\", type=int,\n",
    "        default=None,\n",
    "        help=\"Max training time in seconds (e.g., 120 for 2 minutes)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "     # load your DS config so we can mirror its fp16/bf16 flags in HF\n",
    "    with open(args.deepspeed_config, \"r\") as f:\n",
    "        ds_cfg = json.load(f)\n",
    "    enable_fp16 = bool(ds_cfg.get(\"fp16\", {}).get(\"enabled\", False))\n",
    "    enable_bf16 = bool(ds_cfg.get(\"bf16\", {}).get(\"enabled\", False))\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ðŸš€ Using {torch.cuda.device_count()} GPUs for training!\")\n",
    "\n",
    "    MODEL_PATH = os.getenv(\"MODEL_PATH\", \"EleutherAI/pythia-410m\")\n",
    "    DATASET_HF_PATH = os.getenv(\"DATASET_HF_PATH\", \"xyliu6/deita_6k_processed_cs5260\")\n",
    "    OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "    LOG_DIR = os.getenv(\"LOG_DIR\", \"./logs\")\n",
    "\n",
    "    EPOCHS = int(os.getenv(\"EPOCHS\", 1))\n",
    "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 2))\n",
    "    LEARNING_RATE = float(os.getenv(\"LEARNING_RATE\", 2e-5))\n",
    "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", 1024))\n",
    "\n",
    "    dataset = load_dataset(DATASET_HF_PATH)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.to(device)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        texts = batch.get(\"formatted_text\", [\"\"])\n",
    "        new_texts = [\" \".join(t) if isinstance(t, list) else str(t) for t in texts]\n",
    "        tokenized = tokenizer(\n",
    "            new_texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        tokenized[\"labels\"] = tokenized.input_ids.copy()\n",
    "        return tokenized\n",
    "\n",
    "    tokenized = dataset.map(preprocess, batched=True)\n",
    "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        logging_dir=LOG_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        deepspeed=args.deepspeed_config,\n",
    "        fp16=enable_fp16,\n",
    "        bf16=enable_bf16,\n",
    "        gradient_accumulation_steps=1,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    step_cb = StepTimeCallback()\n",
    "    start_time = time.time()\n",
    "    gpu_thread = threading.Thread(target=gpu_usage_logger, args=(f\"gpu_{args.deepspeed_config}.csv\",2,start_time))\n",
    "    cpu_thread = threading.Thread(target=cpu_usage_logger, args=(f\"cpu_{args.deepspeed_config}.csv\",2,start_time))\n",
    "    gpu_thread.start()\n",
    "    cpu_thread.start()\n",
    "\n",
    "    # build callbacks list\n",
    "    callbacks = [step_cb]\n",
    "    if args.max_time:\n",
    "        time_cb = TimeBasedStoppingCallback(args.max_time)\n",
    "        callbacks.append(time_cb)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        callbacks=callbacks,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    logging_active = False\n",
    "    gpu_thread.join()\n",
    "    cpu_thread.join()\n",
    "\n",
    "    prefix = os.path.splitext(os.path.basename(args.deepspeed_config))[0]\n",
    "    compute_summary(prefix,\n",
    "                    f\"gpu_{args.deepspeed_config}.csv\",\n",
    "                    f\"cpu_{args.deepspeed_config}.csv\",\n",
    "                    f\"step_times_{args.deepspeed_config}.csv\",\n",
    "                    f\"{prefix}_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:32:35.706062Z",
     "iopub.status.busy": "2025-04-25T14:32:35.705426Z",
     "iopub.status.idle": "2025-04-25T14:32:35.733963Z",
     "shell.execute_reply": "2025-04-25T14:32:35.733288Z",
     "shell.execute_reply.started": "2025-04-25T14:32:35.706035Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 1) ZeRO stages 0â€“3 (unquoted EOF so ${stage} expands)\n",
    "for stage in 1 2 3; do\n",
    "  cat > ds_zero_stage${stage}.json << EOF\n",
    "{\n",
    "  \"zero_optimization\": { \"stage\": ${stage} },\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"gradient_accumulation_steps\": \"auto\"\n",
    "}\n",
    "EOF\n",
    "done\n",
    "\n",
    "# 2) Mixed precision\n",
    "cat > ds_fp32.json << 'EOF'\n",
    "{\n",
    "  \"fp16\":   { \"enabled\": false },\n",
    "  \"bf16\":   { \"enabled\": false },\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 1\n",
    "  }\n",
    "}\n",
    "EOF\n",
    "\n",
    "cat > ds_fp16.json << 'EOF'\n",
    "{\n",
    "  \"fp16\":   { \"enabled\": true },\n",
    "  \"bf16\":   { \"enabled\": false },\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 1\n",
    "  }\n",
    "}\n",
    "EOF\n",
    "\n",
    "cat > ds_bf16.json << 'EOF'\n",
    "{\n",
    "  \"fp16\":   { \"enabled\": false },\n",
    "  \"bf16\":   { \"enabled\": true },\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 1\n",
    "  }\n",
    "}\n",
    "EOF\n",
    "\n",
    "\n",
    "# 3) Model parallelism\n",
    "cat > ds_model_parallel.json << EOF\n",
    "{\n",
    "  \"zero_optimization\": { \"stage\": 3 },\n",
    "  \"fp16\": { \"enabled\": true },\n",
    "  \"pipeline\": { \"enabled\": true, \"stages\": 2 },\n",
    "  \"train_batch_size\": \"auto\"\n",
    "}\n",
    "EOF\n",
    "\n",
    "# 4) Offload techniques\n",
    "cat > ds_offload_cpu.json << EOF\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_optimizer\": { \"device\": \"cpu\", \"pin_memory\": true },\n",
    "    \"offload_param\":     { \"device\": \"cpu\", \"pin_memory\": true }\n",
    "  },\n",
    "  \"fp16\": { \"enabled\": true },\n",
    "  \"train_batch_size\": \"auto\"\n",
    "}\n",
    "EOF\n",
    "\n",
    "cat > ds_offload_nvme.json << EOF\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_optimizer\": { \"device\": \"nvme\", \"nvme_path\": \"/nvme/tmp\" },\n",
    "    \"offload_param\":     { \"device\": \"cpu\", \"pin_memory\": true }\n",
    "  },\n",
    "  \"fp16\": { \"enabled\": true },\n",
    "  \"train_batch_size\": \"auto\"\n",
    "}\n",
    "EOF\n",
    "\n",
    "# 5) Large model (Pythia-410m)\n",
    "cat > ds_large_model.json << EOF\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_optimizer\": { \"device\": \"cpu\", \"pin_memory\": true  },\n",
    "    \"offload_param\":     { \"device\": \"cpu\",  \"pin_memory\": true }\n",
    "  },\n",
    "  \"bf16\":     { \"enabled\": true },\n",
    "  \"pipeline\": { \n",
    "      \"enabled\": true, \n",
    "      \"stages\": 2  },\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"gradient_accumulation_steps\": \"auto\"\n",
    "}\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:43:32.041497Z",
     "iopub.status.busy": "2025-04-23T15:43:32.041304Z",
     "iopub.status.idle": "2025-04-23T15:53:54.026033Z",
     "shell.execute_reply": "2025-04-23T15:53:54.024932Z",
     "shell.execute_reply.started": "2025-04-23T15:43:32.041478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_zero_stage1.json ===\n",
      "W0423 15:43:33.751000 2141 torch/distributed/run.py:793] \n",
      "W0423 15:43:33.751000 2141 torch/distributed/run.py:793] *****************************************\n",
      "W0423 15:43:33.751000 2141 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 15:43:33.751000 2141 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 15:43:39.964818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 15:43:39.968431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745423019.993334    2144 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745423020.002077    2144 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745423020.003674    2145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745423020.010748    2145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 15:43:42,562] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 15:43:42,577] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 15:43:48,195] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[2025-04-23 15:43:48,499] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:43:48,499] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  6%|â–ˆâ–ˆâ–Œ                                     | 133/2108 [01:59<29:24,  1.12it/s]^C\n",
      "W0423 15:53:53.621000 2141 torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers\n",
      "W0423 15:53:53.622000 2141 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2144 closing signal SIGINT\n",
      "W0423 15:53:53.622000 2141 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2145 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_zero_stage1.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:31:08.627716Z",
     "iopub.status.busy": "2025-04-23T15:31:08.626943Z",
     "iopub.status.idle": "2025-04-23T15:43:32.039768Z",
     "shell.execute_reply": "2025-04-23T15:43:32.039062Z",
     "shell.execute_reply.started": "2025-04-23T15:31:08.627686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_zero_stage2.json ===\n",
      "W0423 15:31:10.292000 1284 torch/distributed/run.py:793] \n",
      "W0423 15:31:10.292000 1284 torch/distributed/run.py:793] *****************************************\n",
      "W0423 15:31:10.292000 1284 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 15:31:10.292000 1284 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 15:31:16.309313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 15:31:16.309402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745422276.331851    1288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745422276.331996    1287 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745422276.338938    1287 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745422276.338941    1288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 15:31:18,870] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 15:31:18,881] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 15:31:24,691] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:31:24,755] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:31:24,755] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  6%|â–ˆâ–ˆâ–                                     | 130/2108 [01:59<30:37,  1.08it/s][rank0]:[E423 15:43:28.514091122 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=799, OpType=ALLREDUCE, NumelIn=162322944, NumelOut=162322944, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.\n",
      "[rank0]:[E423 15:43:28.524191708 ProcessGroupNCCL.cpp:1785] [PG ID 1 PG GUID 1 Rank 0] Exception (either an error or timeout) detected by watchdog at work: 799, last enqueued NCCL work: 800, last completed NCCL work: 798.\n",
      "[rank0]:[E423 15:43:30.972050538 ProcessGroupNCCL.cpp:1834] [PG ID 1 PG GUID 1 Rank 0] Timeout at NCCL work: 799, last enqueued NCCL work: 800, last completed NCCL work: 798.\n",
      "[rank0]:[E423 15:43:30.972075728 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[rank0]:[E423 15:43:30.972082074 ProcessGroupNCCL.cpp:636] [Rank 0] To avoid data inconsistency, we are taking the entire process down.\n",
      "[rank0]:[E423 15:43:30.973827024 ProcessGroupNCCL.cpp:1595] [PG ID 1 PG GUID 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=799, OpType=ALLREDUCE, NumelIn=162322944, NumelOut=162322944, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.\n",
      "Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e4b33b6c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7e4ae9a19772 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7e4ae9a20bb3 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e4ae9a2261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: <unknown function> + 0x145c0 (0x7e4b33f875c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\n",
      "frame #5: <unknown function> + 0x94ac3 (0x7e4b35302ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #6: clone + 0x44 (0x7e4b35393a04 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "terminate called after throwing an instance of 'c10::DistBackendError'\n",
      "  what():  [PG ID 1 PG GUID 1 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=799, OpType=ALLREDUCE, NumelIn=162322944, NumelOut=162322944, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.\n",
      "Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e4b33b6c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7e4ae9a19772 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7e4ae9a20bb3 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7e4ae9a2261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: <unknown function> + 0x145c0 (0x7e4b33f875c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\n",
      "frame #5: <unknown function> + 0x94ac3 (0x7e4b35302ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #6: clone + 0x44 (0x7e4b35393a04 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7e4b33b6c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0xe4271b (0x7e4ae968f71b in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: <unknown function> + 0x145c0 (0x7e4b33f875c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\n",
      "frame #3: <unknown function> + 0x94ac3 (0x7e4b35302ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #4: clone + 0x44 (0x7e4b35393a04 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "W0423 15:43:31.258000 1284 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1288 closing signal SIGTERM\n",
      "E0423 15:43:31.583000 1284 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 1287) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "=====================================================\n",
      "train_deepspeed.py FAILED\n",
      "-----------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "-----------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-04-23_15:43:31\n",
      "  host      : d0fa0d13ac3c\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : -6 (pid: 1287)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 6 (SIGABRT) received by PID 1287\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_zero_stage2.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:27:27.210897Z",
     "iopub.status.busy": "2025-04-23T15:27:27.210290Z",
     "iopub.status.idle": "2025-04-23T15:29:53.847518Z",
     "shell.execute_reply": "2025-04-23T15:29:53.846607Z",
     "shell.execute_reply.started": "2025-04-23T15:27:27.210873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_zero_stage3.json ===\n",
      "W0423 15:27:28.843000 1012 torch/distributed/run.py:793] \n",
      "W0423 15:27:28.843000 1012 torch/distributed/run.py:793] *****************************************\n",
      "W0423 15:27:28.843000 1012 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 15:27:28.843000 1012 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 15:27:35.122356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 15:27:35.122459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745422055.145251    1016 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745422055.145254    1015 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745422055.152008    1016 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745422055.152011    1015 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 15:27:37,680] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 15:27:37,696] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!ðŸš€ Using 2 GPUs for training!\n",
      "\n",
      "[2025-04-23 15:27:43,912] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:27:43,937] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:27:43,937] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Parameter Offload: Total persistent parameters: 121344 in 98 params\n",
      "{'train_runtime': 120.1188, 'train_samples_per_second': 70.197, 'train_steps_per_second': 17.549, 'train_loss': 2.044157501835151, 'epoch': 0.07}\n",
      "  7%|â–ˆâ–ˆâ–Š                                     | 149/2108 [02:00<26:19,  1.24it/s]\n",
      "Written summary to ds_zero_stage3_summary.txt\n",
      "Written summary to ds_zero_stage3_summary.txt\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_zero_stage*.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:53:54.039939Z",
     "iopub.status.busy": "2025-04-23T15:53:54.039244Z",
     "iopub.status.idle": "2025-04-23T15:57:01.550295Z",
     "shell.execute_reply": "2025-04-23T15:57:01.549176Z",
     "shell.execute_reply.started": "2025-04-23T15:53:54.039907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_fp32.json ===\n",
      "W0423 15:53:55.722000 2879 torch/distributed/run.py:793] \n",
      "W0423 15:53:55.722000 2879 torch/distributed/run.py:793] *****************************************\n",
      "W0423 15:53:55.722000 2879 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 15:53:55.722000 2879 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 15:54:02.088994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 15:54:02.088994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745423642.111798    2883 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745423642.111827    2882 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745423642.119737    2882 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745423642.119822    2883 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 15:54:04,647] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 15:54:04,670] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 15:54:10,985] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:54:10,985] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[2025-04-23 15:54:11,884] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  6%|â–ˆâ–ˆâ–Œ                                     | 133/2108 [01:59<29:15,  1.12it/s]^C\n",
      "W0423 15:57:01.146000 2879 torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers\n",
      "W0423 15:57:01.146000 2879 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2882 closing signal SIGINT\n",
      "W0423 15:57:01.146000 2879 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2883 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_fp32.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:57:07.274187Z",
     "iopub.status.busy": "2025-04-23T15:57:07.273599Z",
     "iopub.status.idle": "2025-04-23T16:00:07.757325Z",
     "shell.execute_reply": "2025-04-23T16:00:07.756198Z",
     "shell.execute_reply.started": "2025-04-23T15:57:07.274156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_fp16.json ===\n",
      "W0423 15:57:08.882000 3186 torch/distributed/run.py:793] \n",
      "W0423 15:57:08.882000 3186 torch/distributed/run.py:793] *****************************************\n",
      "W0423 15:57:08.882000 3186 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 15:57:08.882000 3186 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 15:57:15.002001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 15:57:15.002065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745423835.024238    3189 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745423835.024323    3190 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745423835.031079    3189 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745423835.031278    3190 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 15:57:17,634] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 15:57:17,708] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 15:57:23,310] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[2025-04-23 15:57:23,819] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 15:57:23,819] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 368/2108 [01:59<09:43,  2.98it/s]^C\n",
      "W0423 16:00:07.353000 3186 torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers\n",
      "W0423 16:00:07.353000 3186 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3189 closing signal SIGINT\n",
      "W0423 16:00:07.353000 3186 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3190 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_fp16.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:00:13.332532Z",
     "iopub.status.busy": "2025-04-23T16:00:13.331852Z",
     "iopub.status.idle": "2025-04-23T16:03:44.121694Z",
     "shell.execute_reply": "2025-04-23T16:03:44.120459Z",
     "shell.execute_reply.started": "2025-04-23T16:00:13.332502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_bf16.json ===\n",
      "W0423 16:00:14.963000 3487 torch/distributed/run.py:793] \n",
      "W0423 16:00:14.963000 3487 torch/distributed/run.py:793] *****************************************\n",
      "W0423 16:00:14.963000 3487 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 16:00:14.963000 3487 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 16:00:21.060589: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 16:00:21.060611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745424021.083545    3490 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745424021.083546    3491 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745424021.090502    3490 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745424021.090528    3491 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 16:00:23,609] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 16:00:23,616] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 16:00:29,496] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 16:00:29,496] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-04-23 16:00:29,511] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  6%|â–ˆâ–ˆâ–                                     | 116/2108 [01:59<35:17,  1.06s/it]^C\n",
      "W0423 16:03:43.717000 3487 torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers\n",
      "W0423 16:03:43.717000 3487 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3490 closing signal SIGINT\n",
      "W0423 16:03:43.718000 3487 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3491 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_bf16.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:03:47.898349Z",
     "iopub.status.busy": "2025-04-23T16:03:47.898053Z",
     "iopub.status.idle": "2025-04-23T16:06:42.712417Z",
     "shell.execute_reply": "2025-04-23T16:06:42.711431Z",
     "shell.execute_reply.started": "2025-04-23T16:03:47.898324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_model_parallel.json ===\n",
      "W0423 16:03:49.519000 3820 torch/distributed/run.py:793] \n",
      "W0423 16:03:49.519000 3820 torch/distributed/run.py:793] *****************************************\n",
      "W0423 16:03:49.519000 3820 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 16:03:49.519000 3820 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 16:03:55.697494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 16:03:55.697498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745424235.719953    3824 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745424235.720092    3823 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745424235.726836    3824 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745424235.726839    3823 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 16:03:58,259] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 16:03:58,264] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 16:04:03,832] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 16:04:03,895] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 16:04:03,895] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Parameter Offload: Total persistent parameters: 121344 in 98 params\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 404/2108 [01:59<08:23,  3.39it/s]^C\n",
      "W0423 16:06:42.308000 3820 torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers\n",
      "W0423 16:06:42.308000 3820 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3823 closing signal SIGINT\n",
      "W0423 16:06:42.309000 3820 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3824 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_model_parallel.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:06:48.398398Z",
     "iopub.status.busy": "2025-04-23T16:06:48.397771Z",
     "iopub.status.idle": "2025-04-23T16:09:59.881292Z",
     "shell.execute_reply": "2025-04-23T16:09:59.880573Z",
     "shell.execute_reply.started": "2025-04-23T16:06:48.398372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running ds_offload_cpu.json ===\n",
      "W0423 16:06:50.108000 4124 torch/distributed/run.py:793] \n",
      "W0423 16:06:50.108000 4124 torch/distributed/run.py:793] *****************************************\n",
      "W0423 16:06:50.108000 4124 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0423 16:06:50.108000 4124 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-23 16:06:56.637234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745424416.661057    4127 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745424416.668380    4127 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-23 16:06:56.810867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745424416.834816    4128 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745424416.842991    4128 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-23 16:06:59,368] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-23 16:06:59,543] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "[2025-04-23 16:07:05,504] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-23 16:07:05,504] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[2025-04-23 16:07:05,722] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Installed CUDA version 12.5 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu124/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Installed CUDA version 12.5 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 40.29740810394287 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 40.058308839797974 seconds\n",
      "Parameter Offload: Total persistent parameters: 121344 in 98 params\n",
      "{'train_runtime': 120.5781, 'train_samples_per_second': 69.93, 'train_steps_per_second': 17.482, 'train_loss': 2.08248838444346, 'epoch': 0.05}\n",
      "  5%|â–ˆâ–‰                                       | 97/2108 [02:00<41:39,  1.24s/it]\n",
      "Written summary to ds_offload_cpu_summary.txt\n",
      "Written summary to ds_offload_cpu_summary.txt\n"
     ]
    }
   ],
   "source": [
    "!for cfg in ds_offload_cpu.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:32:38.654485Z",
     "iopub.status.busy": "2025-04-25T14:32:38.653761Z",
     "iopub.status.idle": "2025-04-25T14:35:58.898265Z",
     "shell.execute_reply": "2025-04-25T14:35:58.897545Z",
     "shell.execute_reply.started": "2025-04-25T14:32:38.654459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n",
      "=== Running ds_large_model.json ===\n",
      "W0425 14:32:43.451000 428 torch/distributed/run.py:793] \n",
      "W0425 14:32:43.451000 428 torch/distributed/run.py:793] *****************************************\n",
      "W0425 14:32:43.451000 428 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0425 14:32:43.451000 428 torch/distributed/run.py:793] *****************************************\n",
      "2025-04-25 14:32:49.491384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-25 14:32:49.491548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745591569.513701     434 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745591569.513699     435 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745591569.520770     435 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1745591569.520773     434 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[2025-04-25 14:32:52,253] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-04-25 14:32:52,253] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "ðŸš€ Using 2 GPUs for training!\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 192.59 examples/s]\n",
      "[2025-04-25 14:32:59,201] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-04-25 14:32:59,201] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 217.52 examples/s]\n",
      "[2025-04-25 14:32:59,352] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "/kaggle/working/train_deepspeed.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Installed CUDA version 12.5 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu124/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Installed CUDA version 12.5 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 40.41240191459656 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 40.2427499294281 seconds\n",
      "Parameter Offload: Total persistent parameters: 321536 in 194 params\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                                  | 0/2108 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                        | 1/2108 [00:04<2:44:49,  4.69s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                        | 2/2108 [00:08<2:29:26,  4.26s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                        | 3/2108 [00:11<2:11:27,  3.75s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                        | 4/2108 [00:14<2:03:26,  3.52s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                        | 5/2108 [00:18<1:59:14,  3.40s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|                                        | 6/2108 [00:21<1:56:29,  3.33s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|â–                                       | 7/2108 [00:24<1:54:43,  3.28s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|â–                                       | 8/2108 [00:27<1:53:53,  3.25s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|â–                                       | 9/2108 [00:30<1:53:21,  3.24s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  0%|â–                                      | 10/2108 [00:34<1:52:41,  3.22s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 11/2108 [00:37<1:52:47,  3.23s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 12/2108 [00:40<1:53:15,  3.24s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 13/2108 [00:43<1:53:28,  3.25s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 14/2108 [00:47<1:53:45,  3.26s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 15/2108 [00:50<1:54:11,  3.27s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 16/2108 [00:53<1:54:51,  3.29s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 17/2108 [00:57<1:55:23,  3.31s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 18/2108 [01:00<1:55:34,  3.32s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 19/2108 [01:03<1:55:58,  3.33s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Ž                                      | 20/2108 [01:07<1:56:17,  3.34s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 21/2108 [01:10<1:56:49,  3.36s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 22/2108 [01:14<1:57:26,  3.38s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 23/2108 [01:17<1:58:19,  3.41s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 24/2108 [01:20<1:58:58,  3.43s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 25/2108 [01:24<1:59:33,  3.44s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 26/2108 [01:28<2:02:52,  3.54s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–                                      | 27/2108 [01:31<2:02:42,  3.54s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Œ                                      | 28/2108 [01:35<2:03:03,  3.55s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Œ                                      | 29/2108 [01:38<2:03:43,  3.57s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Œ                                      | 30/2108 [01:42<2:04:16,  3.59s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  1%|â–Œ                                      | 31/2108 [01:46<2:05:08,  3.61s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  2%|â–Œ                                      | 32/2108 [01:49<2:06:02,  3.64s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  2%|â–Œ                                      | 33/2108 [01:53<2:06:41,  3.66s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "  2%|â–‹                                      | 34/2108 [01:57<2:07:38,  3.69s/it]/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "{'train_runtime': 121.1811, 'train_samples_per_second': 69.582, 'train_steps_per_second': 17.395, 'train_loss': 1.630757359095982, 'epoch': 0.02}\n",
      "  2%|â–‹                                      | 35/2108 [02:01<1:59:37,  3.46s/it]\n",
      "[Summary Error] Could not compute summary for ds_large_model: [Errno 2] No such file or directory: 'step_times_ds_large_model.json.csv'\n",
      "[Summary Error] Could not compute summary for ds_large_model: [Errno 2] No such file or directory: 'step_times_ds_large_model.json.csv'\n"
     ]
    }
   ],
   "source": [
    "# 1) Install a NumPy 1.24.x wheel\n",
    "!pip install numpy==1.24.4\n",
    "\n",
    "# 2) Restart your Kaggle session (so the new NumPy is actually loaded)\n",
    "\n",
    "!for cfg in ds_large_model.json; do \\\n",
    "    echo \"=== Running $cfg ===\"; \\\n",
    "    CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 \\\n",
    "      train_deepspeed.py --deepspeed_config $cfg --max_time 120; \\\n",
    "  done"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
