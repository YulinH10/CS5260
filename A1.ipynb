{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "```!pip install openai numpy tqdm tiktoken```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from heapq import nlargest\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"(?s).*#### \", \"\", text)\n",
    "    text = re.sub(r\"\\.$\", \"\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    \n",
    "    if not text:\n",
    "        return \"-1000000000\"\n",
    "    if \".\" in text:  \n",
    "        try:\n",
    "            return str(int(float(text)))  # Convert \"4.0\" to \"4\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "    if text.isdigit():  \n",
    "        return text\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_value(text: str) -> str:\n",
    "    pattern = r\"(-?[$0-9.,]{2,})|(-?[0-9]+)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        for match_groups in matches[::-1]:\n",
    "            for group in match_groups:\n",
    "                if group:\n",
    "                    return clean_text(group)\n",
    "    \n",
    "    return \"-1000000000\"\n",
    "\n",
    "def load_dataset(file_path: str, sample_size: int = 20) -> List[Dict]:\n",
    "    \"\"\"Load and sample from dataset.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return random.sample(data, sample_size)\n",
    "\n",
    "def count_tokens(text: str, model: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def evaluate_accuracy(predictions: List[str], ground_truth: List[str]) -> float:\n",
    "    \"\"\"Calculate accuracy of predictions.\"\"\"\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        try:\n",
    "            if int(pred) == int(truth):\n",
    "                correct += 1\n",
    "        except:\n",
    "            pass\n",
    "    return correct / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thoughts Implementation\n",
    "\n",
    "1. Visit [DeepInfra](https://deepinfra.com/) and **register an account**. Familiarize yourself with how to use the API by referring to the [documentation](https://deepinfra.com/docs).  \n",
    "\n",
    "2. Test the **API call** functionality provided by DeepInfra to ensure proper integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThought:\n",
    "    def __init__(self, api_key: str, base_url: str = \"https://api.deepinfra.com/v1/openai\",\n",
    "                 model: str = \"Qwen/Qwen2.5-7B-Instruct\", temperature: float = 0.3):\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def solve(self, question: str) -> Tuple[str, int]:\n",
    "        prompt = f\"\"\"You are an excellent math problem solver. Your task is to solve the following math problem step by step. \n",
    "        Follow these instructions carefully:\n",
    "\n",
    "        1. Understand the Problem: Read the problem carefully and identify what is being asked. Break down the problem into smaller parts if necessary.\n",
    "        2. Plan the Solution: Decide on the mathematical concepts, formulas, or strategies that will be used to solve the problem.\n",
    "        3. Execute the Plan: Perform the necessary calculations step by step. Introduce intermediate variables or assumptions if needed.\n",
    "        4. Verify Each Step: After each step, explain the reasoning process of each step, check the correctness and confidence of the calculations and reasoning. If it's wrong, correct any errors immediately.\n",
    "        5. Reflect on the Solution: After solving the problem, review the steps to ensure the solution is logical and accurate. Summarize the reasoning process.\n",
    "\n",
    "        Problem: {question}\n",
    "\n",
    "        Let's think step by step and ensure each step is correct before moving to the next one.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature,\n",
    "                max_completion_tokens=2048,\n",
    "            )\n",
    "            tokens_used = response.usage.total_tokens\n",
    "            self.total_tokens += tokens_used\n",
    "            return response.choices[0].message.content, tokens_used\n",
    "        except Exception as e:\n",
    "            print(f\"Error in API call: {e}\")\n",
    "            return \"\", 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-Thoughts Implementation\n",
    "\n",
    "1. You are *highly encouraged* to read the [original paper](http://arxiv.org/abs/2501.02497) and run the [codebase](https://github.com/princeton-nlp/tree-of-thought-llm) first.  \n",
    "    - Otherwise, you may have no idea what ToT is doing.\n",
    "    \n",
    "    - For simplicity, start by running the basic configuration:\n",
    "        - Search algorithm: BFS\n",
    "\n",
    "        - Thought generator: propose prompt\n",
    "\n",
    "        - Task: Game of 24\n",
    "\n",
    "2. Regarding your own implementation below, feel free to experiment with various hyperparameters, including:\n",
    "    - API call parameters (e.g., `temperature`)\n",
    "    \n",
    "    - ToT implementation parameters (e.g., `max_steps`, `n_samples_per_step`)\n",
    "\n",
    "3. [Optional] You could try other **7B-level** base models instead of `Qwen2.5-7B-Instruct` in DeepInfra. You might achieve better results with proper implementation.\n",
    "\n",
    "4. [Optional] Multi-model setups are also allowedâ€”for example, using one model to generate thoughts and another to evaluate them (reward model?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import openai\n",
    "from heapq import nlargest\n",
    "from openai import OpenAI\n",
    "\n",
    "class TreeOfThoughts:\n",
    "    def __init__(self, api_key: str, base_url: str = \"https://api.deepinfra.com/v1/openai\",\n",
    "                 model: str = \"Qwen/Qwen2.5-7B-Instruct\", temperature: float = 0.3):\n",
    "        \"\"\"Initialize the Tree-of-Thoughts solver.\"\"\"\n",
    "        # TODO: Initialize the OpenAI client and other necessary attributes\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.evaluation_cache: Dict[str, float] = {}\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def chat_with_gpt(self, prompt: str, n: int = 1, stop: str = None) -> Tuple[List[str], int]:\n",
    "        \"\"\"Get completions from GPT model.\"\"\"\n",
    "        # [IMPORTANT] `stop` is important here. You can refer to the implementation of the Game of 24 in the original ToT codebase.\n",
    "        # TODO: Implement the chat completion function\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=self.temperature,\n",
    "                n=n,\n",
    "                max_completion_tokens=2048,\n",
    "                stop=stop\n",
    "            )\n",
    "            tokens_used = response.usage.total_tokens\n",
    "            self.total_tokens += tokens_used\n",
    "            return [choice.message.content for choice in response.choices], tokens_used\n",
    "        except Exception as e:\n",
    "            print(f\"Error in chat_with_gpt: {e}\")\n",
    "            return [], 0\n",
    "\n",
    "    def generate_thoughts(self, question: str, current_thought: str = \"\", n_samples: int = 3) -> List[str]:\n",
    "        \"\"\"Generate multiple possible next steps in reasoning.\"\"\"\n",
    "        # [IMPORTANT] A one-shot example can help the model follow your instructions precisely.\n",
    "        # TODO: Implement thought generation function\n",
    "        prompt = f\"\"\"\n",
    "        You are an excellent math problem solver. Solve the following problem with precise step-by-step reasoning.\n",
    "        \n",
    "        Problem: {question}\n",
    "        Current Thought Process: {current_thought}\n",
    "\n",
    "        Follow these instructions carefully:\n",
    "\n",
    "        1. Understand the Problem: Read the problem carefully and identify what is being asked. Break down the problem into smaller step-by-step calculations if necessary. Clearly define the given information.\n",
    "        2. Plan the Solution: Identify the required solution and relevant formulas. Decide on the mathematical concepts, formulas, or strategies that will be used to solve the problem.\n",
    "        3. Execute the Plan: Perform the necessary calculations step by step accurately. Introduce intermediate variables or assumptions if needed. Output the final numerical answer in the correct format: just the number without additional text.\n",
    "        4. Verify Each Step: After each step, explain the reasoning process of each step, check the correctness and confidence of the calculations and reasoning. If it's wrong, correct any errors immediately.\n",
    "        5. Reflect on the Solution: After solving the problem, review the steps to ensure the solution is logical and accurate. Summarize the reasoning process.\n",
    "\n",
    "        \n",
    "        Let's think step by step strictly and ensure each step is correct before moving to the next one.\n",
    "        \"\"\"\n",
    "        thoughts, tokens_used = self.chat_with_gpt(prompt, n=n_samples)\n",
    "        self.total_tokens += tokens_used\n",
    "        return thoughts\n",
    "\n",
    "    def evaluate_thought(self, question: str, thought: str) -> float:\n",
    "        \"\"\"Evaluate the likelihood that a thought process leads to the correct answer.\"\"\"\n",
    "        # [IMPORTANT] You can use the base model (Qwen2.5-7B-Instruct) for self-evaluation; however, it is not the only option.\n",
    "        # TODO: Implement thought evaluation function\n",
    "        if thought in self.evaluation_cache:\n",
    "            return self.evaluation_cache[thought]\n",
    "        \n",
    "        prompt = f\"\"\"Rate how likely this reasoning process will lead to the correct solution:\n",
    "        Question: {question}\n",
    "        Reasoning: {thought}\n",
    "        Rate from 0 to 1, where 0 = wrong approach, 1 = correct approach.\n",
    "        Return only the number.\"\"\"\n",
    "        \n",
    "        response, _ = self.chat_with_gpt(prompt, n=1)\n",
    "        try:\n",
    "            score = float(response[0].strip())\n",
    "            score = max(0.0, min(1.0, score))  # Clamp score between 0 and 1\n",
    "            self.evaluation_cache[thought] = score\n",
    "            return score\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def select_best_thoughts(self, thoughts: List[str], scores: List[float], k: int = 2) -> List[str]:\n",
    "        \"\"\"Select the k best thoughts based on their scores.\"\"\"\n",
    "        # TODO: Implement thought selection function\n",
    "        return [thought for thought, _ in nlargest(k, zip(thoughts, scores), key=lambda x: x[1])]\n",
    "\n",
    "    def self_correct(self, question: str, solution: str) -> str:\n",
    "        \"\"\"Prompt the model to self-correct its solution.\"\"\"\n",
    "        prompt = f\"\"\"Review the following solution, reasoning, summary and correct any mistakes:\n",
    "        Question: {question}\n",
    "        Solution: {solution}\n",
    "        \n",
    "        Instructions:\n",
    "        - Identify and fix any calculation errors.\n",
    "        - Ensure all steps follow a logical sequence.\n",
    "        - Ensure the final numerical answer is properly rounded.\n",
    "        - Output only the corrected final numerical answer without additional text.\n",
    "        \"\"\"\n",
    "        corrected_solution, _ = self.chat_with_gpt(prompt, n=1)\n",
    "        return corrected_solution[0] if corrected_solution else solution\n",
    "\n",
    "    \n",
    "    \n",
    "    def solve(self, question: str, max_steps: int = 8, n_samples_per_step: int = 3, k_best_thoughts: int = 2) -> str:\n",
    "        \"\"\"Solve a problem using Tree-of-Thoughts reasoning.\"\"\"\n",
    "        # TODO: Implement the main solving function\n",
    "        # current_thoughts = [\"Initial thought: Let's solve this step by step.\"]\n",
    "        current_thoughts=['''You are an excellent math problem solver. Solve the following problem with precise step-by-step reasoning. Let's think step by step strictly and ensure each step is correct before moving to the next one.''']\n",
    "        for step in range(max_steps):\n",
    "            new_thoughts = []\n",
    "            for thought in current_thoughts:\n",
    "                generated_thoughts = self.generate_thoughts(question, thought, n_samples_per_step)\n",
    "                new_thoughts.extend([\n",
    "                    f\"{thought}\\nStep {step + 1}: {new_thought}\"\n",
    "                    for new_thought in generated_thoughts\n",
    "                ])\n",
    "            \n",
    "            if not new_thoughts:\n",
    "                break\n",
    "            \n",
    "            scores = [self.evaluate_thought(question, thought) for thought in new_thoughts]\n",
    "            current_thoughts = self.select_best_thoughts(new_thoughts, scores, k_best_thoughts)\n",
    "            \n",
    "            if step == max_steps - 1:  # Only correct at the final step\n",
    "                current_thoughts = [self.self_correct(question, thought) for thought in current_thoughts]\n",
    "            \n",
    "            if max(scores, default=0) > 0.95:\n",
    "                return current_thoughts[0]\n",
    "        \n",
    "        return current_thoughts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ToT Using One Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: You are an excellent math problem solver. Solve the following problem with precise step-by-step reasoning. Let's think step by step strictly and ensure each step is correct before moving to the next one.\n",
      "Step 1: ### Step-by-Step Solution\n",
      "\n",
      "1. **Understand the Problem:**\n",
      "   - John initially has 5 apples.\n",
      "   - John buys 3 more apples.\n",
      "   - We need to find out how many apples John has in total.\n",
      "\n",
      "2. **Plan the Solution:**\n",
      "   - We will use simple addition to find the total number of apples.\n",
      "   - The formula is: Total apples = Initial apples + Apples bought.\n",
      "\n",
      "3. **Execute the Plan:**\n",
      "   - Initial apples = 5\n",
      "   - Apples bought = 3\n",
      "   - Total apples = 5 + 3\n",
      "\n",
      "4. **Perform the Calculation:**\n",
      "   - 5 + 3 = 8\n",
      "\n",
      "5. **Verify Each Step:**\n",
      "   - Initial apples (5) is correct.\n",
      "   - Apples bought (3) is correct.\n",
      "   - Adding 5 and 3 gives 8, which is correct.\n",
      "\n",
      "6. **Reflect on the Solution:**\n",
      "   - The steps are logical and the calculation is straightforward.\n",
      "   - The final answer is 8.\n",
      "\n",
      "**Final Answer:**\n",
      "John has 8 apples.\n",
      "Extracted answer: 8\n"
     ]
    }
   ],
   "source": [
    "def test_tot():\n",
    "    # Set your API key\n",
    "    api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "    if not api_key:\n",
    "        print(\"Please set DEEPINFRA_TOKEN environment variable\")\n",
    "        return\n",
    "\n",
    "    # Test with a single example\n",
    "    test_question = \"If John has 5 apples and buys 3 more, how many apples does he have?\"\n",
    "    \n",
    "    tot_solver = TreeOfThoughts(api_key)\n",
    "    solution = tot_solver.solve(\n",
    "        question=test_question,\n",
    "        max_steps=20,\n",
    "        n_samples_per_step=4,\n",
    "        k_best_thoughts=2\n",
    "    )\n",
    "    \n",
    "    answer = extract_value(solution)\n",
    "    print(\"Solution:\", solution)\n",
    "    print(\"Extracted answer:\", answer)\n",
    "\n",
    "test_tot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on the Validation Set\n",
    "\n",
    "- You can use multithreading to accelerate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:48<00:00, 22.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Results ===\n",
      "ToT Accuracy: 91.00%\n",
      "CoT Accuracy: 85.00%\n",
      "ToT Total Tokens: 733225\n",
      "CoT Total Tokens: 81181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. load DeepInfra api key\n",
    "api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "\n",
    "# 2. Load and sample dataset\n",
    "dataset = load_dataset(\"cs5260_val_random300.jsonl\", sample_size=100)\n",
    "\n",
    "# 3. Initialize reasoning solvers\n",
    "tot_solver = TreeOfThoughts(api_key)\n",
    "cot_solver = ChainOfThought(api_key)\n",
    "\n",
    "\n",
    "def process_batch(batch_data):\n",
    "    tot_results, cot_results = [], []\n",
    "    tot_tokens, cot_tokens = 0, 0\n",
    "    \n",
    "    for item in batch_data:\n",
    "        question = item[\"question\"]\n",
    "        true_answer = item[\"answer\"]\n",
    "        \n",
    "        # ToT solving\n",
    "        tot_solution = tot_solver.solve(\n",
    "            question=question,\n",
    "            max_steps=8, \n",
    "            n_samples_per_step=4, \n",
    "            k_best_thoughts=2 \n",
    "        )\n",
    "        tot_answer = extract_value(tot_solution)\n",
    "        tot_results.append({\n",
    "            \"question_id\": item[\"question_id\"],\n",
    "            \"predicted\": tot_answer,\n",
    "            \"true\": true_answer,\n",
    "        })\n",
    "            \n",
    "        # # Check if ToT answer is incorrect and print details\n",
    "        # if tot_answer != true_answer:\n",
    "        #     print(f\"ToT Answer Mismatch - Question ID: {item['question_id']}\")\n",
    "        #     print(f\"Question: {question}\")\n",
    "        #     print(f\"Correct Answer: {true_answer}\")\n",
    "        #     print(f\"Generated Answer: {tot_answer}\")\n",
    "        #     print(f\"Generated Answer Token Count: {len(tot_solution)}\")  # Assuming 'tot_solution' is a list or string containing tokens\n",
    "        \n",
    "        # CoT solving\n",
    "        cot_solution, tokens = cot_solver.solve(question)\n",
    "        cot_answer = extract_value(cot_solution)\n",
    "        cot_results.append({\n",
    "            \"question_id\": item[\"question_id\"],\n",
    "            \"predicted\": cot_answer,\n",
    "            \"true\": true_answer,\n",
    "        })\n",
    "        cot_tokens += tokens\n",
    "            \n",
    "        # # Check if CoT answer is incorrect and print details\n",
    "        # if cot_answer != true_answer:\n",
    "        #     print(f\"CoT Answer Mismatch - Question ID: {item['question_id']}\")\n",
    "        #     print(f\"Question: {question}\")\n",
    "        #     print(f\"Correct Answer: {true_answer}\")\n",
    "        #     print(f\"Generated Answer: {cot_answer}\")\n",
    "        #     print(f\"Generated Answer Token Count: {tokens}\")  # Assuming 'tokens' is the number of tokens generated\n",
    "    \n",
    "    return tot_results, cot_results, tot_tokens, cot_tokens\n",
    "\n",
    "\n",
    "# 5. Process dataset using multithreading\n",
    "batch_size = 10\n",
    "num_threads = 10\n",
    "batch_data = [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]\n",
    "\n",
    "tot_results, cot_results = [], []\n",
    "tot_tokens, cot_tokens = 0, 0\n",
    "\n",
    "# Use ThreadPoolExecutor to process the batches in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(process_batch, batch) for batch in batch_data]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        batch_tot_results, batch_cot_results,  batch_tot_tokens, batch_cot_tokens = future.result()\n",
    "        \n",
    "        tot_results.extend(batch_tot_results)\n",
    "        cot_results.extend(batch_cot_results)\n",
    "        tot_tokens += batch_tot_tokens\n",
    "        cot_tokens += batch_cot_tokens\n",
    "\n",
    "# Calculate metrics\n",
    "tot_accuracy = evaluate_accuracy([r[\"predicted\"] for r in tot_results], \n",
    "                                [r[\"true\"] for r in tot_results])\n",
    "cot_accuracy = evaluate_accuracy([r[\"predicted\"] for r in cot_results], \n",
    "                                [r[\"true\"] for r in cot_results])\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(f\"ToT Accuracy: {tot_accuracy:.2%}\")\n",
    "print(f\"CoT Accuracy: {cot_accuracy:.2%}\")\n",
    "print(f\"ToT Total Tokens: {tot_solver.total_tokens}\")\n",
    "print(f\"CoT Total Tokens: {cot_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "- Refer to [here](https://www.kaggle.com/competitions/cs-5260-spring-2025-assignment-1/data) for submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set and build submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [05:32<00:00, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation complete. Results saved to sample_submission.csv\n",
      "Total tokens used: 2736064\n",
      "Total questions processed: 300\n",
      "Submission format verification passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from typing import List, Dict, Any\n",
    "import csv\n",
    "\n",
    "def evaluate_test_set(\n",
    "    test_filepath: str,\n",
    "    tot_solver: TreeOfThoughts,\n",
    "    output_filepath: str = \"sample_submission.csv\",\n",
    "    num_threads: int = 30\n",
    ") -> None:\n",
    "    print(\"\\nProcessing test set...\")\n",
    "    test_data = load_dataset(test_filepath, sample_size=300)\n",
    "    \n",
    "    # Verify test data format\n",
    "    if not all(item[\"question_id\"].startswith(\"test_\") for item in test_data):\n",
    "        raise ValueError(\"Invalid test data format: question_ids should start with 'test_'\")\n",
    "    \n",
    "    # Create batches of data\n",
    "    batch_size = len(test_data) // num_threads\n",
    "    batches = [test_data[i:i + batch_size] for i in range(0, len(test_data), batch_size)]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Function to process a batch of questions\n",
    "    def process_batch(batch_data):\n",
    "        batch_results = []\n",
    "        for item in batch_data:\n",
    "            question = item[\"question\"]\n",
    "            question_id = item[\"question_id\"]\n",
    "            \n",
    "            try:\n",
    "                # Get ToT solution\n",
    "                tot_solution = tot_solver.solve(\n",
    "                    question=question,\n",
    "                    max_steps=8,\n",
    "                    n_samples_per_step=3,\n",
    "                    k_best_thoughts=2\n",
    "                )\n",
    "                tot_answer = extract_value(tot_solution)\n",
    "                \n",
    "                # Handle None/invalid answers\n",
    "                if tot_answer is None:\n",
    "                    print(f\"Warning: Could not extract numerical answer for {question_id}\")\n",
    "                    tot_answer = 0  # Default value for invalid answers\n",
    "                \n",
    "                batch_results.append({\n",
    "                    \"question_id\": question_id,\n",
    "                    \"answer\": tot_answer\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {question_id}: {str(e)}\")\n",
    "                # Include failed questions with default value\n",
    "                batch_results.append({\n",
    "                    \"question_id\": question_id,\n",
    "                    \"answer\": 0\n",
    "                })\n",
    "        \n",
    "        return batch_results\n",
    "\n",
    "    # Use ThreadPoolExecutor to process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(process_batch, batch) for batch in batches]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            results.extend(future.result())\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_filepath, index=False)\n",
    "    \n",
    "    print(f\"\\nTest set evaluation complete. Results saved to {output_filepath}\")\n",
    "    print(f\"Total tokens used: {tot_solver.total_tokens}\")\n",
    "    print(f\"Total questions processed: {len(results)}\")\n",
    "    \n",
    "    # Verify submission format\n",
    "    verify_submission_format(output_filepath)\n",
    "\n",
    "def verify_submission_format(filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the submission file meets the required format.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the submission CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Check column names\n",
    "    required_columns = [\"question_id\", \"answer\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Submission must contain columns: {required_columns}\")\n",
    "    \n",
    "    # Check question_id format\n",
    "    if not all(str(qid).startswith(\"test_\") for qid in df[\"question_id\"]):\n",
    "        raise ValueError(\"All question_ids must start with 'test_'\")\n",
    "    \n",
    "    # Check for duplicate question_ids\n",
    "    if len(df[\"question_id\"]) != len(df[\"question_id\"].unique()):\n",
    "        raise ValueError(\"Duplicate question_ids found in submission\")\n",
    "    \n",
    "    # Check that answers are numeric\n",
    "    if not pd.to_numeric(df[\"answer\"], errors=\"coerce\").notnull().all():\n",
    "        raise ValueError(\"All answers must be numeric values\")\n",
    "    \n",
    "    print(\"Submission format verification passed!\")\n",
    "\n",
    "# Evaluate on test set and create submission file\n",
    "test_filepath = \"cs5260_test_random300.jsonl\"\n",
    "output_filepath = \"sample_submission.csv\"\n",
    "evaluate_test_set(test_filepath, tot_solver, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
